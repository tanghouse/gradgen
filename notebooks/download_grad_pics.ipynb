{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b119eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, time, json, sys, argparse, random\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import urllib.robotparser as robotparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43879873",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://churchillgowns.com\"\n",
    "INDEX_URL = f\"{BASE}/pages/select-your-university\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; uni-scraper/1.0; +https://example.org/)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "DEGREE_KEYWORDS = {\n",
    "    # Map common keywords to your desired folder names\n",
    "    \"bsc\": \"BSc\",\n",
    "    \"ba \": \"BA\",\n",
    "    \"bachelor\": \"Bachelors\",\n",
    "    \"msc\": \"MSc\",\n",
    "    \"ms \": \"MSc\",\n",
    "    \"ma \": \"MA\",\n",
    "    \"master\": \"Masters\",\n",
    "    \"doctoral\": \"Doctoral\",\n",
    "    \"phd\": \"Doctoral\",\n",
    "}\n",
    "\n",
    "COLLECTION_FILTERS = {\n",
    "    # Which collections to follow from each uni page\n",
    "    # Keys are simple contains-matches on the collection title\n",
    "    \"hire\": \"Graduation Set (Hire)\",\n",
    "    \"buy\": \"Full Graduation Set\"\n",
    "}\n",
    "\n",
    "def polite_get(session, url, robots, delay=(0.5, 1.2), retries=3, backoff=1.6):\n",
    "    if not robots.can_fetch(HEADERS[\"User-Agent\"], url):\n",
    "        raise PermissionError(f\"robots.txt disallows fetch: {url}\")\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            resp = session.get(url, headers=HEADERS, timeout=20)\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                raise requests.HTTPError(f\"retryable status {resp.status_code}\")\n",
    "            resp.raise_for_status()\n",
    "            time.sleep(random.uniform(*delay))\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            if i == retries - 1:\n",
    "                raise\n",
    "            time.sleep((backoff ** i) + random.random())\n",
    "\n",
    "def sanitize(name):\n",
    "    name = re.sub(r\"\\s+\", \" \", name.strip())\n",
    "    name = re.sub(r\"[\\\\/:*?\\\"<>|]\", \"_\", name)  # Win-safe\n",
    "    return name\n",
    "\n",
    "def infer_degree_folder(text):\n",
    "    t = text.lower() + \" \"\n",
    "    for k, v in DEGREE_KEYWORDS.items():\n",
    "        if k in t:\n",
    "            return v\n",
    "    return \"Unknown\"\n",
    "\n",
    "def parse_university_links(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = []\n",
    "    # Links are rendered as headings with anchor; grab all anchors under the “Select Your University” listing\n",
    "    for a in soup.select(\"a[href*='/pages/']\"):\n",
    "        href = a.get(\"href\") or \"\"\n",
    "        text = a.get_text(strip=True)\n",
    "        if \"/pages/\" in href and text and \"university\" in text.lower():\n",
    "            links.append((text, urljoin(BASE, href)))\n",
    "    # De-dup and preserve order\n",
    "    seen, out = set(), []\n",
    "    for name, href in links:\n",
    "        if href not in seen:\n",
    "            seen.add(href)\n",
    "            out.append((name, href))\n",
    "    return out\n",
    "\n",
    "def find_degree_collections(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    items = []\n",
    "    # university “Select Your Package” tiles link to collections\n",
    "    for a in soup.select(\"a[href*='/collections/']\"):\n",
    "        title = a.get_text(\" \", strip=True)\n",
    "        href = urljoin(BASE, a.get(\"href\"))\n",
    "        if title:\n",
    "            items.append((title, href))\n",
    "    return items\n",
    "\n",
    "def extract_product_links(collection_html):\n",
    "    soup = BeautifulSoup(collection_html, \"html.parser\")\n",
    "    prods = []\n",
    "    for a in soup.select(\"a[href*='/products/']\"):\n",
    "        title = a.get_text(\" \", strip=True)\n",
    "        href = urljoin(BASE, a.get(\"href\"))\n",
    "        if title:\n",
    "            prods.append((title, href))\n",
    "    # de-dup\n",
    "    seen, out = set(), []\n",
    "    for t, h in prods:\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            out.append((t, h))\n",
    "    return out\n",
    "\n",
    "def extract_image_urls(product_html):\n",
    "    soup = BeautifulSoup(product_html, \"html.parser\")\n",
    "    urls = set()\n",
    "    # 1) typical Shopify product image <img> or <meta property=\"og:image\">\n",
    "    for tag in soup.select(\"img[src], source[srcset], meta[property='og:image']\"):\n",
    "        if tag.name == \"meta\":\n",
    "            src = tag.get(\"content\")\n",
    "        else:\n",
    "            src = tag.get(\"src\") or tag.get(\"srcset\")\n",
    "            if src and \" \" in src and \",\" in src:\n",
    "                # srcset: take first URL\n",
    "                src = src.split(\",\")[0].strip().split(\" \")[0]\n",
    "        if not src:\n",
    "            continue\n",
    "        full = src if src.startswith(\"http\") else urljoin(BASE, src)\n",
    "        # Filter for Shopify CDN images or site-hosted images likely relevant\n",
    "        if any(k in full for k in (\"cdn.shopify.com\", \"churchillgowns.com\")):\n",
    "            urls.add(full)\n",
    "    return sorted(urls)\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--out\", default=\"output\", help=\"Output folder\")\n",
    "    ap.add_argument(\"--include\", nargs=\"+\", default=[\"hire\", \"buy\"],\n",
    "                    help=\"Which collection types to include: hire, buy\")\n",
    "    ap.add_argument(\"--max-unis\", type=int, default=9999, help=\"Limit universities crawled\")\n",
    "    ap.add_argument(\"--dry-run\", action=\"store_true\", help=\"List actions without downloading\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    outdir = Path(args.out)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # robots.txt\n",
    "    robots = robotparser.RobotFileParser()\n",
    "    robots.set_url(urljoin(BASE, \"/robots.txt\"))\n",
    "    try:\n",
    "        robots.read()\n",
    "    except Exception:\n",
    "        print(\"Warning: could not read robots.txt; proceed with caution.\", file=sys.stderr)\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    # 1) index -> list universities\n",
    "    idx = polite_get(session, INDEX_URL, robots)\n",
    "    unis = parse_university_links(idx.text)\n",
    "    if not unis:\n",
    "        print(\"No universities parsed — site structure might have changed.\", file=sys.stderr)\n",
    "        return\n",
    "\n",
    "    # Manifest\n",
    "    manifest_path = outdir / \"manifest.csv\"\n",
    "    mf = open(manifest_path, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "    writer = csv.writer(mf)\n",
    "    writer.writerow([\"university\", \"degree_folder\", \"product_title\", \"image_url\", \"saved_path\"])\n",
    "\n",
    "    uni_count = 0\n",
    "    for uni_name, uni_url in unis:\n",
    "        uni_count += 1\n",
    "        if uni_count > args.max_unis:\n",
    "            break\n",
    "        u_name = sanitize(uni_name)\n",
    "        print(f\"\\n== {uni_name} == {uni_url}\")\n",
    "\n",
    "        try:\n",
    "            uresp = polite_get(session, uni_url, robots)\n",
    "        except PermissionError as e:\n",
    "            print(f\"Skip (robots): {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Skip (fetch error): {e}\")\n",
    "            continue\n",
    "\n",
    "        collections = find_degree_collections(uresp.text)\n",
    "        # Filter by include types (hire/buy) using title contains heuristics\n",
    "        filtered = []\n",
    "        for title, href in collections:\n",
    "            t = title.lower()\n",
    "            take = False\n",
    "            if \"hire\" in args.include and \"hire\" in t:\n",
    "                take = True\n",
    "            if \"buy\" in args.include and (\"buy\" in t or \"purchase\" in t):\n",
    "                take = True\n",
    "            if take:\n",
    "                filtered.append((title, href))\n",
    "\n",
    "        for coll_title, coll_href in filtered:\n",
    "            try:\n",
    "                cresp = polite_get(session, coll_href, robots)\n",
    "            except Exception as e:\n",
    "                print(f\"  - Skip collection ({coll_title}): {e}\")\n",
    "                continue\n",
    "            products = extract_product_links(cresp.text)\n",
    "            if not products:\n",
    "                # some collections may render products via JS; still try the collection page images\n",
    "                products = [(coll_title, coll_href)]\n",
    "\n",
    "            for prod_title, prod_href in products:\n",
    "                try:\n",
    "                    presp = polite_get(session, prod_href, robots)\n",
    "                except Exception as e:\n",
    "                    print(f\"    - Skip product ({prod_title}): {e}\")\n",
    "                    continue\n",
    "\n",
    "                imgs = extract_image_urls(presp.text)\n",
    "                if not imgs:\n",
    "                    continue\n",
    "\n",
    "                degree_folder = infer_degree_folder(prod_title + \" \" + coll_title)\n",
    "                # Normalize specific Masters/Bachelors into MA/MSc/BA/BSc if keywords found\n",
    "                # (We keep Masters/Bachelors if subtype not present)\n",
    "                pf_uni = outdir / u_name / degree_folder\n",
    "                pf_uni.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                for i, url in enumerate(imgs, 1):\n",
    "                    # Make filename from product + index\n",
    "                    ext = os.path.splitext(urlparse(url).path)[1] or \".jpg\"\n",
    "                    fname = sanitize(f\"{prod_title}-{i}\")[:150] + ext\n",
    "                    dest = pf_uni / fname\n",
    "\n",
    "                    if args.dry_run:\n",
    "                        print(f\"DRY-RUN save: {dest} <- {url}\")\n",
    "                        writer.writerow([uni_name, degree_folder, prod_title, url, str(dest)])\n",
    "                        continue\n",
    "\n",
    "                    if dest.exists():\n",
    "                        writer.writerow([uni_name, degree_folder, prod_title, url, str(dest)])\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        rimg = polite_get(session, url, robots, delay=(0.3, 0.8))\n",
    "                        with open(dest, \"wb\") as f:\n",
    "                            f.write(rimg.content)\n",
    "                        print(f\"Saved: {dest.name}\")\n",
    "                        writer.writerow([uni_name, degree_folder, prod_title, url, str(dest)])\n",
    "                    except Exception as e:\n",
    "                        print(f\"      - Image failed: {url} :: {e}\")\n",
    "\n",
    "    mf.close()\n",
    "    print(f\"\\nDone. Manifest: {manifest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a81e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--out OUT]\n",
      "                             [--include INCLUDE [INCLUDE ...]]\n",
      "                             [--max-unis MAX_UNIS] [--dry-run]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=\"/Users/Haichen Shi/Library/Jupyter/runtime/kernel-v3445293826b2734c48148186687630e4636daeafd.json\"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haichen Shi/Library/Caches/pypoetry/virtualenvs/tanghouse-S8DqrOLL-py3.13/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838b2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tanghouse-S8DqrOLL-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
